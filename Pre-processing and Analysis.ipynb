{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b720532",
   "metadata": {},
   "source": [
    "**Importing the necessary libraries and loading the data into pandas DataFrames:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de2caafa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\S\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\S\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\S\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\S\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7c1706",
   "metadata": {},
   "source": [
    "**Loading the data into a dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27a001e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the data into a dataframes\n",
    "Thailand_travel = pd.read_csv('C:/Users/S/MDA/Text Analytics/Project/Preprocessing and Analysis/reddit data/Thailand_travel.csv')\n",
    "travel = pd.read_csv('C:/Users/S/MDA/Text Analytics/Project/Preprocessing and Analysis/reddit data/travel.csv')\n",
    "traveldeals = pd.read_csv('C:/Users/S/MDA/Text Analytics/Project/Preprocessing and Analysis/reddit data/traveldeals.csv')\n",
    "travelhacks = pd.read_csv('C:/Users/S/MDA/Text Analytics/Project/Preprocessing and Analysis/reddit data/travelhacks.csv')\n",
    "travelphotos = pd.read_csv('C:/Users/S/MDA/Text Analytics/Project/Preprocessing and Analysis/reddit data/travelphotos.csv')\n",
    "adventures = pd.read_csv('C:/Users/S/MDA/Text Analytics/Project/Preprocessing and Analysis/reddit data/adventures.csv')\n",
    "airbnb = pd.read_csv('C:/Users/S/MDA/Text Analytics/Project/Preprocessing and Analysis/reddit data/airbnb.csv')\n",
    "Australia_travel = pd.read_csv('C:/Users/S/MDA/Text Analytics/Project/Preprocessing and Analysis/reddit data/Australia_travel.csv')\n",
    "backpacking = pd.read_csv('C:/Users/S/MDA/Text Analytics/Project/Preprocessing and Analysis/reddit data/backpacking.csv')\n",
    "campingandhiking = pd.read_csv('C:/Users/S/MDA/Text Analytics/Project/Preprocessing and Analysis/reddit data/campingandhiking.csv')\n",
    "Canada_travel = pd.read_csv('C:/Users/S/MDA/Text Analytics/Project/Preprocessing and Analysis/reddit data/Canada_travel.csv')\n",
    "Costa_Rica_travel = pd.read_csv('C:/Users/S/MDA/Text Analytics/Project/Preprocessing and Analysis/reddit data/Costa Rica_travel.csv')\n",
    "Dubai_travel = pd.read_csv('C:/Users/S/MDA/Text Analytics/Project/Preprocessing and Analysis/reddit data/Dubai_travel.csv')\n",
    "Egypt_travel = pd.read_csv('C:/Users/S/MDA/Text Analytics/Project/Preprocessing and Analysis/reddit data/Egypt_travel.csv')\n",
    "England_travel = pd.read_csv('C:/Users/S/MDA/Text Analytics/Project/Preprocessing and Analysis/reddit data/England_travel.csv')\n",
    "France_travel = pd.read_csv('C:/Users/S/MDA/Text Analytics/Project/Preprocessing and Analysis/reddit data/France_travel.csv')\n",
    "Greece_travel = pd.read_csv('C:/Users/S/MDA/Text Analytics/Project/Preprocessing and Analysis/reddit data/Greece_travel.csv')\n",
    "Hong_Kong_travel = pd.read_csv('C:/Users/S/MDA/Text Analytics/Project/Preprocessing and Analysis/reddit data/Hong Kong_travel.csv')\n",
    "Iceland_travel = pd.read_csv('C:/Users/S/MDA/Text Analytics/Project/Preprocessing and Analysis/reddit data/Iceland_travel.csv')\n",
    "Indonesia_travel = pd.read_csv('C:/Users/S/MDA/Text Analytics/Project/Preprocessing and Analysis/reddit data/Indonesia_travel.csv')\n",
    "Ä°stanbul_travel = pd.read_csv('C:/Users/S/MDA/Text Analytics/Project/Preprocessing and Analysis/reddit data/Ä°stanbul_travel.csv')\n",
    "Italy_travel = pd.read_csv('C:/Users/S/MDA/Text Analytics/Project/Preprocessing and Analysis/reddit data/Italy_travel.csv')\n",
    "Japan_travel = pd.read_csv('C:/Users/S/MDA/Text Analytics/Project/Preprocessing and Analysis/reddit data/Japan_travel.csv')\n",
    "London_travel = pd.read_csv('C:/Users/S/MDA/Text Analytics/Project/Preprocessing and Analysis/reddit data/London_travel.csv')\n",
    "Maldives_travel = pd.read_csv('C:/Users/S/MDA/Text Analytics/Project/Preprocessing and Analysis/reddit data/Maldives_travel.csv')\n",
    "Mexico_travel = pd.read_csv('C:/Users/S/MDA/Text Analytics/Project/Preprocessing and Analysis/reddit data/Mexico_travel.csv')\n",
    "New_York_City_travel = pd.read_csv('C:/Users/S/MDA/Text Analytics/Project/Preprocessing and Analysis/reddit data/New York City_travel.csv')\n",
    "New_Zealand_travel = pd.read_csv('C:/Users/S/MDA/Text Analytics/Project/Preprocessing and Analysis/reddit data/New Zealand_travel.csv')\n",
    "Paris_travel = pd.read_csv('C:/Users/S/MDA/Text Analytics/Project/Preprocessing and Analysis/reddit data/Paris_travel.csv')\n",
    "roadtrip = pd.read_csv('C:/Users/S/MDA/Text Analytics/Project/Preprocessing and Analysis/reddit data/roadtrip.csv')\n",
    "Rome_travel = pd.read_csv('C:/Users/S/MDA/Text Analytics/Project/Preprocessing and Analysis/reddit data/Rome_travel.csv')\n",
    "Singapore_travel = pd.read_csv('C:/Users/S/MDA/Text Analytics/Project/Preprocessing and Analysis/reddit data/Singapore_travel.csv')\n",
    "solotravel = pd.read_csv('C:/Users/S/MDA/Text Analytics/Project/Preprocessing and Analysis/reddit data/solotravel.csv')\n",
    "South_Africa_travel = pd.read_csv('C:/Users/S/MDA/Text Analytics/Project/Preprocessing and Analysis/reddit data/South Africa_travel.csv')\n",
    "Switzerland_travel = pd.read_csv('C:/Users/S/MDA/Text Analytics/Project/Preprocessing and Analysis/reddit data/Switzerland_travel.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89dd19d8",
   "metadata": {},
   "source": [
    "**Preprocessing the data by cleaning the text, standardizing dates, and tokenizing the data:**\n",
    "\n",
    "The preprocess_text function converts the text to lowercase, removes URLs, non-alphanumeric characters, and extra whitespaces. It then tokenizes the text and removes stopwords using NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25dd2f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean and preprocess the text data\n",
    "def preprocess_text(text):\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
    "    \n",
    "    # Remove non-alphanumeric characters and extra whitespaces\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    # Join the tokens into a string\n",
    "    processed_text = ' '.join(tokens)\n",
    "    \n",
    "    return processed_text\n",
    "\n",
    "# Apply preprocessing to the datasets\n",
    "Thailand_travel['cleaned_text']  = Thailand_travel['title'].apply(preprocess_text)\n",
    "travel['cleaned_text']  = travel['title'].apply(preprocess_text)\n",
    "traveldeals['cleaned_text']  = traveldeals['title'].apply(preprocess_text)\n",
    "travelhacks['cleaned_text']  = travelhacks['title'].apply(preprocess_text)\n",
    "travelphotos['cleaned_text']  = travelphotos['title'].apply(preprocess_text)\n",
    "adventures['cleaned_text']  = adventures['title'].apply(preprocess_text)\n",
    "airbnb['cleaned_text']  = airbnb['title'].apply(preprocess_text)\n",
    "Australia_travel['cleaned_text']  = Australia_travel['title'].apply(preprocess_text)\n",
    "backpacking['cleaned_text']  = backpacking['title'].apply(preprocess_text)\n",
    "campingandhiking['cleaned_text']  = campingandhiking['title'].apply(preprocess_text)\n",
    "Canada_travel['cleaned_text']  = Canada_travel['title'].apply(preprocess_text)\n",
    "Costa_Rica_travel['cleaned_text']  = Costa_Rica_travel['title'].apply(preprocess_text)\n",
    "Dubai_travel['cleaned_text']  = Dubai_travel['title'].apply(preprocess_text)\n",
    "Egypt_travel['cleaned_text']  = Egypt_travel['title'].apply(preprocess_text)\n",
    "England_travel['cleaned_text']  = England_travel['title'].apply(preprocess_text)\n",
    "France_travel['cleaned_text']  = France_travel['title'].apply(preprocess_text)\n",
    "Greece_travel['cleaned_text']  = Greece_travel['title'].apply(preprocess_text)\n",
    "Hong_Kong_travel['cleaned_text']  = Hong_Kong_travel['title'].apply(preprocess_text)\n",
    "Iceland_travel['cleaned_text']  = Iceland_travel['title'].apply(preprocess_text)\n",
    "Indonesia_travel['cleaned_text']  = Indonesia_travel['title'].apply(preprocess_text)\n",
    "Ä°stanbul_travel['cleaned_text']  = Ä°stanbul_travel['title'].apply(preprocess_text)\n",
    "Italy_travel['cleaned_text']  = Italy_travel['title'].apply(preprocess_text)\n",
    "Japan_travel['cleaned_text']  = Japan_travel['title'].apply(preprocess_text)\n",
    "London_travel['cleaned_text']  = London_travel['title'].apply(preprocess_text)\n",
    "Maldives_travel['cleaned_text']  = Maldives_travel['title'].apply(preprocess_text)\n",
    "Mexico_travel['cleaned_text']  = Mexico_travel['title'].apply(preprocess_text)\n",
    "New_York_City_travel['cleaned_text']  = New_York_City_travel['title'].apply(preprocess_text)\n",
    "New_Zealand_travel['cleaned_text']  = New_Zealand_travel['title'].apply(preprocess_text)\n",
    "Paris_travel['cleaned_text']  = Paris_travel['title'].apply(preprocess_text)\n",
    "roadtrip['cleaned_text']  = roadtrip['title'].apply(preprocess_text)\n",
    "Rome_travel['cleaned_text']  = Rome_travel['title'].apply(preprocess_text)\n",
    "Singapore_travel['cleaned_text']  = Singapore_travel['title'].apply(preprocess_text)\n",
    "solotravel['cleaned_text']  = solotravel['title'].apply(preprocess_text)\n",
    "South_Africa_travel['cleaned_text']  = South_Africa_travel['title'].apply(preprocess_text)\n",
    "Switzerland_travel['cleaned_text']  = Switzerland_travel['title'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a94d9c",
   "metadata": {},
   "source": [
    "**1. Performing Sentiment Analysis**\n",
    "\n",
    "**Performing sentiment analysis on the preprocessed text using the VADER (Valence Aware Dictionary and sEntiment Reasoner) sentiment analysis tool from NLTK:**\n",
    "\n",
    "The classify_sentiment function utilizes the SentimentIntensityAnalyzer from NLTK to calculate the sentiment score for each comment. Based on the compound score, it classifies the comment as positive, negative, or neutral.\n",
    "\n",
    "After executing the below code, the DataFrame will have two additional columns: 'cleaned_text', which contains the preprocessed tokenized text, and 'sentiment', which represents the sentiment classification for each comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e99553a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the SentimentIntensityAnalyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Function to perform sentiment analysis and classify comments\n",
    "def classify_sentiment(text):\n",
    "    sentiment_score = sia.polarity_scores(text)\n",
    "    if sentiment_score['compound'] >= 0.05:\n",
    "        return 'Positive'\n",
    "    elif sentiment_score['compound'] <= -0.05:\n",
    "        return 'Negative'\n",
    "    else:\n",
    "        return 'Neutral'\n",
    "\n",
    "# Apply sentiment analysis to the datasets\n",
    "Thailand_travel['sentiment'] = Thailand_travel['cleaned_text'].apply(classify_sentiment)\n",
    "travel['sentiment'] = travel['cleaned_text'].apply(classify_sentiment)\n",
    "traveldeals['sentiment'] = traveldeals['cleaned_text'].apply(classify_sentiment)\n",
    "travelhacks['sentiment'] = travelhacks['cleaned_text'].apply(classify_sentiment)\n",
    "travelphotos['sentiment'] = travelphotos['cleaned_text'].apply(classify_sentiment)\n",
    "adventures['sentiment'] = adventures['cleaned_text'].apply(classify_sentiment)\n",
    "airbnb['sentiment'] = airbnb['cleaned_text'].apply(classify_sentiment)\n",
    "Australia_travel['sentiment'] = Australia_travel['cleaned_text'].apply(classify_sentiment)\n",
    "backpacking['sentiment'] = backpacking['cleaned_text'].apply(classify_sentiment)\n",
    "campingandhiking['sentiment'] = campingandhiking['cleaned_text'].apply(classify_sentiment)\n",
    "Canada_travel['sentiment'] = Canada_travel['cleaned_text'].apply(classify_sentiment)\n",
    "Costa_Rica_travel['sentiment'] = Costa_Rica_travel['cleaned_text'].apply(classify_sentiment)\n",
    "Dubai_travel['sentiment'] = Dubai_travel['cleaned_text'].apply(classify_sentiment)\n",
    "Egypt_travel['sentiment'] = Egypt_travel['cleaned_text'].apply(classify_sentiment)\n",
    "England_travel['sentiment'] = England_travel['cleaned_text'].apply(classify_sentiment)\n",
    "France_travel['sentiment'] = France_travel['cleaned_text'].apply(classify_sentiment)\n",
    "Greece_travel['sentiment'] = Greece_travel['cleaned_text'].apply(classify_sentiment)\n",
    "Hong_Kong_travel['sentiment'] = Hong_Kong_travel['cleaned_text'].apply(classify_sentiment)\n",
    "Iceland_travel['sentiment'] = Iceland_travel['cleaned_text'].apply(classify_sentiment)\n",
    "Indonesia_travel['sentiment'] = Indonesia_travel['cleaned_text'].apply(classify_sentiment)\n",
    "Ä°stanbul_travel['sentiment'] = Ä°stanbul_travel['cleaned_text'].apply(classify_sentiment)\n",
    "Italy_travel['sentiment'] = Italy_travel['cleaned_text'].apply(classify_sentiment)\n",
    "Japan_travel['sentiment'] = Japan_travel['cleaned_text'].apply(classify_sentiment)\n",
    "London_travel['sentiment'] = London_travel['cleaned_text'].apply(classify_sentiment)\n",
    "Maldives_travel['sentiment'] = Maldives_travel['cleaned_text'].apply(classify_sentiment)\n",
    "Mexico_travel['sentiment'] = Mexico_travel['cleaned_text'].apply(classify_sentiment)\n",
    "New_York_City_travel['sentiment'] = New_York_City_travel['cleaned_text'].apply(classify_sentiment)\n",
    "New_Zealand_travel['sentiment'] = New_Zealand_travel['cleaned_text'].apply(classify_sentiment)\n",
    "Paris_travel['sentiment'] = Paris_travel['cleaned_text'].apply(classify_sentiment)\n",
    "roadtrip['sentiment'] = roadtrip['cleaned_text'].apply(classify_sentiment)\n",
    "Rome_travel['sentiment'] = Rome_travel['cleaned_text'].apply(classify_sentiment)\n",
    "Singapore_travel['sentiment'] = Singapore_travel['cleaned_text'].apply(classify_sentiment)\n",
    "solotravel['sentiment'] = solotravel['cleaned_text'].apply(classify_sentiment)\n",
    "South_Africa_travel['sentiment'] = South_Africa_travel['cleaned_text'].apply(classify_sentiment)\n",
    "Switzerland_travel['sentiment'] = Switzerland_travel['cleaned_text'].apply(classify_sentiment)\n",
    "\n",
    "# Save the dataframes as a CSV files\n",
    "Thailand_travel.to_csv('processed_Thailand_travel_with_sentiment_analysis.csv', index=False)\n",
    "travel.to_csv('processed_travel_with_sentiment_analysis.csv', index=False)\n",
    "traveldeals.to_csv('processed_traveldeals_with_sentiment_analysis.csv', index=False)\n",
    "travelhacks.to_csv('processed_travelhacks_with_sentiment_analysis.csv', index=False)\n",
    "travelphotos.to_csv('processed_travelphotos_with_sentiment_analysis.csv', index=False)\n",
    "adventures.to_csv('processed_adventures_with_sentiment_analysis.csv', index=False)\n",
    "airbnb.to_csv('processed_airbnb_with_sentiment_analysis.csv', index=False)\n",
    "Australia_travel.to_csv('processed_Australia_travel_with_sentiment_analysis.csv', index=False)\n",
    "backpacking.to_csv('processed_backpacking_with_sentiment_analysis.csv', index=False)\n",
    "campingandhiking.to_csv('processed_campingandhiking_with_sentiment_analysis.csv', index=False)\n",
    "Canada_travel.to_csv('processed_Canada_travel_with_sentiment_analysis.csv', index=False)\n",
    "Costa_Rica_travel.to_csv('processed_Costa_Rica_travel_with_sentiment_analysis.csv', index=False)\n",
    "Dubai_travel.to_csv('processed_Dubai_travel_with_sentiment_analysis.csv', index=False)\n",
    "Egypt_travel.to_csv('processed_Egypt_travel_with_sentiment_analysis.csv', index=False)\n",
    "England_travel.to_csv('processed_England_travel_with_sentiment_analysis.csv', index=False)\n",
    "France_travel.to_csv('processed_France_travel_with_sentiment_analysis.csv', index=False)\n",
    "Greece_travel.to_csv('processed_Greece_travel_with_sentiment_analysis.csv', index=False)\n",
    "Hong_Kong_travel.to_csv('processed_Hong_Kong_travel_with_sentiment_analysis.csv', index=False)\n",
    "Iceland_travel.to_csv('processed_Iceland_travel_with_sentiment_analysis.csv', index=False)\n",
    "Indonesia_travel.to_csv('processed_Indonesia_travel_with_sentiment_analysis.csv', index=False)\n",
    "Ä°stanbul_travel.to_csv('processed_Ä°stanbul_travel_with_sentiment_analysis.csv', index=False)\n",
    "Italy_travel.to_csv('processed_Italy_travel_with_sentiment_analysis.csv', index=False)\n",
    "Japan_travel.to_csv('processed_Japan_travel_with_sentiment_analysis.csv', index=False)\n",
    "London_travel.to_csv('processed_London_travel_with_sentiment_analysis.csv', index=False)\n",
    "Maldives_travel.to_csv('processed_Maldives_travel_with_sentiment_analysis.csv', index=False)\n",
    "Mexico_travel.to_csv('processed_Mexico_travel_with_sentiment_analysis.csv', index=False)\n",
    "New_York_City_travel.to_csv('processed_New_York_City_travel_with_sentiment_analysis.csv', index=False)\n",
    "New_Zealand_travel.to_csv('processed_New_Zealand_travel_with_sentiment_analysis.csv', index=False)\n",
    "Paris_travel.to_csv('processed_Paris_travel_with_sentiment_analysis.csv', index=False)\n",
    "roadtrip.to_csv('processed_roadtrip_with_sentiment_analysis.csv', index=False)\n",
    "Rome_travel.to_csv('processed_Rome_travel_with_sentiment_analysis.csv', index=False)\n",
    "Singapore_travel.to_csv('processed_Singapore_travel_with_sentiment_analysis.csv', index=False)\n",
    "solotravel.to_csv('processed_solotravel_with_sentiment_analysis.csv', index=False)\n",
    "South_Africa_travel.to_csv('processed_South_Africa_travel_with_sentiment_analysis.csv', index=False)\n",
    "Switzerland_travel.to_csv('processed_Switzerland_travel_with_sentiment_analysis.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83c1fa1",
   "metadata": {},
   "source": [
    "**2. Performing Topic Modeling:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63c51cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# Create a TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer(max_features=1000)\n",
    "\n",
    "# Fit and transform the text data\n",
    "tfidf_matrix = vectorizer.fit_transform(reddit_data['cleaned_text'])\n",
    "\n",
    "# Perform topic modeling using Latent Dirichlet Allocation (LDA)\n",
    "lda = LatentDirichletAllocation(n_components=5, random_state=42)\n",
    "topics = lda.fit_transform(tfidf_matrix)\n",
    "\n",
    "# Get the top words in each topic\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "topic_words = []\n",
    "for idx, topic in enumerate(lda.components_):\n",
    "    top_words = [feature_names[i] for i in topic.argsort()[:-6:-1]]\n",
    "    topic_words.append(top_words)\n",
    "\n",
    "# Create a DataFrame for the topic modeling results\n",
    "topic_modeling_results = pd.DataFrame({'Topic': [f\"Topic {idx+1}\" for idx in range(len(topic_words))],\n",
    "                                       'Top Words': topic_words})\n",
    "\n",
    "# Save the topic modeling results to a CSV file\n",
    "topic_modeling_results.to_csv('topic_modeling_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9faef92e",
   "metadata": {},
   "source": [
    "**3. Performing Aspect-based Sentiment Analysis:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f104aec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package opinion_lexicon to\n",
      "[nltk_data]     C:\\Users\\S\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package opinion_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('opinion_lexicon')\n",
    "from nltk.corpus import opinion_lexicon\n",
    "\n",
    "\n",
    "# Create a set of positive and negative words\n",
    "positive_words = set(opinion_lexicon.positive())\n",
    "negative_words = set(opinion_lexicon.negative())\n",
    "\n",
    "# Function to determine the sentiment of an aspect\n",
    "def get_aspect_sentiment(aspect):\n",
    "    words = aspect.split()\n",
    "    pos_count = len(positive_words.intersection(words))\n",
    "    neg_count = len(negative_words.intersection(words))\n",
    "    \n",
    "    if pos_count > neg_count:\n",
    "        return 'Positive'\n",
    "    elif neg_count > pos_count:\n",
    "        return 'Negative'\n",
    "    else:\n",
    "        return 'Neutral'\n",
    "\n",
    "# Apply aspect-based sentiment analysis to the comments\n",
    "reddit_data['aspect_sentiment'] = reddit_data['title'].apply(get_aspect_sentiment)\n",
    "\n",
    "# Save the aspect-based sentiment analysis results to a CSV file\n",
    "reddit_data.to_csv('aspect_sentiment_analysis_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aae9fec",
   "metadata": {},
   "source": [
    "**4. Predict Customer Satisfaction:**\n",
    "\n",
    "To predict customer satisfaction, you can use machine learning algorithms such as regression or random forest. You'll need a labeled dataset where customer satisfaction levels are already known. You can use the sentiment scores obtained from the sentiment analysis as target variables and other features such as aspect mentions or comment length as input variables. Here are examples using random forest, ogistic regression and support vector machine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec17b82c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: : 0.8383575883575883\n",
      "Logistic Regression Accuracy: 0.8165280665280665\n",
      "SVM Accuracy: 0.8227650727650727\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Prepare the data\n",
    "X = tfidf_matrix.toarray()\n",
    "y = reddit_data['sentiment']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Train a random forest classifier\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Random Forest Accuracy: : {accuracy}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Train a logistic regression classifier\n",
    "lr = LogisticRegression(random_state=42, max_iter=1000)  # Increase the max_iter value\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_lr = lr.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy_lr = accuracy_score(y_test, y_pred_lr)\n",
    "print(f\"Logistic Regression Accuracy: {accuracy_lr}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Train a support vector machine classifier\n",
    "svm = SVC(random_state=42)\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_svm = svm.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
    "print(f\"SVM Accuracy: {accuracy_svm}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0b5198",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
